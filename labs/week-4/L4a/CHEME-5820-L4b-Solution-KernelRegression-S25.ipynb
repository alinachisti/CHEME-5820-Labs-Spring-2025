{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32bb413-ee5a-42ee-a758-db6241bab369",
   "metadata": {},
   "source": [
    "# L4b: Kernel Regression of Financial Time Series Data\n",
    "In this lab, we will use [kernel regression](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote14.html) to construct a model of the daily growth rate of an unobserved firm in terms of other members of the [S&P500 index](https://en.wikipedia.org/wiki/S%26P_500). In particular, we'll compute the time series of a specified firm's growth rate using data from a defined list of (observed) basis firms. \n",
    "\n",
    "_Hmmm. This sounds familiar._ This idea is similar in spirit to a [state observer from control theory](https://en.wikipedia.org/wiki/State_observer), in the sense we are using (observed) measurements of some states (the growth rate of our basis firms) to estimate the values of (theoretically) unobserved states. However, instead of using a [linear state space model](https://en.wikipedia.org/wiki/State-space_representation), we'll use kernel regression.\n",
    "\n",
    "### Tasks\n",
    "Before we start, divide into teams and familiarize yourself with the lab. Then, execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "* __Task 1: Review Theory of Kernel Regression (10 min)__: Let's take a few minutes to review the theory behind kernel regression and talk about what we can do with it. \n",
    "* __Task 2: Setup, Data, and Prerequisites (10 min)__: Nextm, we explore the [Open-High-Low-Close (OHLC) dataset](https://en.wikipedia.org/wiki/Open-high-low-close_chart) we'll explore today. We'll load the full dataset, do some cleanup, scale it, and then split the scaled data into training and test datasets.\n",
    "* __Task 3: Build, train and test a kernel regression machine (30 min)__: In this task, we'll use the growth rate data for the tickers in a `basis_tickers::Array{String,1}` array, and data for an unknown (to be specified) ticker to estimate the expansion weights $\\alpha$. We'll then freeze these weights and perform a growth rate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da311201-3ddd-48ba-9798-f7d600a1a2c8",
   "metadata": {},
   "source": [
    "## Task 1: Review Theory of Kernel Regression\n",
    "Suppose we have a dataset $\\mathcal{D} = \\{(\\mathbf{x}_{i},y_{i}) \\mid i = 1,2,\\dots,n\\}$, where the vectors $\\mathbf{x}_i \\in \\mathbb{R}^{m}$ are $m$-dimensional feature vectors ($m\\ll{n}$) and the target variables are continuous values $y_i \\in \\mathbb{R}$, e.g., the price of a house, the price of a stock, the temperature, etc. We can model the relationship between the output, e.g., price, and the features as a linear regression problem: $\\hat{\\mathbf{y}} = \\hat{\\mathbf{X}}\\theta$ where $\\hat{\\mathbf{X}}\\in\\mathbb{R}^{n\\times{p}}$ is a data matrix with the augmented feature vectors $\\hat{\\mathbf{x}}^{\\top}$ on the rows, and $\\theta\\in\\mathbb{R}^{p}$ where $p = m+1$ is an unknown parameter vector. The (regularized) least squares solution for the parameters $\\theta$ is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{\\theta}}_{\\lambda} = \\left(\\hat{\\mathbf{X}}^{\\top}\\hat{\\mathbf{X}}+\\lambda\\,\\mathbf{I}\\right)^{-1}\\hat{\\mathbf{X}}^{\\top}\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The basic idea of kernel regression is to rewrite the parameter vector $\\hat{\\theta}_{\\lambda}$ as a weighted sum of the _augmented training feature variables_: $\\hat{\\theta}_{\\lambda} \\equiv \\sum_{i=1}^{n}\\alpha_{i}\\hat{\\mathbf{x}}_{i}$. Then for some (new, unknown) feature vector $\\hat{\\mathbf{z}}$,  the predicted output $\\hat{y}$ is given by:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} & = \\hat{\\mathbf{z}}^{\\top}\\theta = \\sum_{i=1}^{n}\\alpha_{i}\\left<\\hat{\\mathbf{z}},\\mathbf{x}_{i}\\right>\\quad\\mid\\text{\\,Replace the inner product with a kernel}\\\\\n",
    "        & = \\hat{\\mathbf{z}}^{\\top}\\theta \\simeq \\sum_{i=1}^{n}\\alpha_{i}\\,k(\\hat{\\mathbf{z}},\\mathbf{x}_{i})\n",
    "\\end{align}\n",
    "$$\n",
    "where $k(\\hat{\\mathbf{z}},\\mathbf{x}_{i})$ denotes a kernel function (similarity score) between a new (augmented) feature vector $\\hat{\\mathbf{z}}$ and the (known) training feature vector $\\hat{\\mathbf{x}}_{i}$. The $\\alpha_{i}$ parameters have a nice [analytical solution](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-4/L4a/docs/Notes.pdf) in terms of the _training data_:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha = \\left(\\mathbf{K}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{y}\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{K}$ is the kernel matrix with elements $K_{ij} = k(\\mathbf{v}_{i},\\mathbf{v}_{j})$, the matrix $\\mathbf{I}$ denotes the identity matrix, the vector $\\mathbf{y}$ denotes the _training_ outputs and $\\lambda\\geq{0}$ denotes a regularization parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba092c15-d2e7-4bb5-acb7-db2b3d617d6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d9375e-542d-466d-8017-fd4ec6407163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a/Project.toml`\n",
      "  \u001b[90m[336ed68f] \u001b[39m\u001b[92m+ CSV v0.10.15\u001b[39m\n",
      "  \u001b[90m[a93c6f00] \u001b[39m\u001b[92m+ DataFrames v1.7.0\u001b[39m\n",
      "  \u001b[90m[31c24e10] \u001b[39m\u001b[92m+ Distributions v0.25.117\u001b[39m\n",
      "  \u001b[90m[5789e2e9] \u001b[39m\u001b[92m+ FileIO v1.16.6\u001b[39m\n",
      "  \u001b[90m[033835bb] \u001b[39m\u001b[92m+ JLD2 v0.5.11\u001b[39m\n",
      "  \u001b[90m[ec8451be] \u001b[39m\u001b[92m+ KernelFunctions v0.10.64\u001b[39m\n",
      "  \u001b[90m[91a5bcdd] \u001b[39m\u001b[92m+ Plots v1.40.9\u001b[39m\n",
      "  \u001b[90m[08abe8d2] \u001b[39m\u001b[92m+ PrettyTables v2.4.0\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n",
      "  \u001b[90m[f3b207a7] \u001b[39m\u001b[92m+ StatsPlots v0.15.7\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[93m~ LinearAlgebra â‡’ v1.11.0\u001b[39m\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a/Manifest.toml`\n",
      "  \u001b[90m[621f4979] \u001b[39m\u001b[92m+ AbstractFFTs v1.5.0\u001b[39m\n",
      "  \u001b[90m[79e6a3ab] \u001b[39m\u001b[92m+ Adapt v4.1.1\u001b[39m\n",
      "  \u001b[90m[66dad0bd] \u001b[39m\u001b[92m+ AliasTables v1.1.3\u001b[39m\n",
      "  \u001b[90m[7d9fca2a] \u001b[39m\u001b[92m+ Arpack v0.5.4\u001b[39m\n",
      "  \u001b[90m[13072b0f] \u001b[39m\u001b[92m+ AxisAlgorithms v1.1.0\u001b[39m\n",
      "  \u001b[90m[d1d4a3ce] \u001b[39m\u001b[92m+ BitFlags v0.1.9\u001b[39m\n",
      "  \u001b[90m[336ed68f] \u001b[39m\u001b[92m+ CSV v0.10.15\u001b[39m\n",
      "  \u001b[90m[d360d2e6] \u001b[39m\u001b[92m+ ChainRulesCore v1.25.1\u001b[39m\n",
      "  \u001b[90m[aaaa29a8] \u001b[39m\u001b[92m+ Clustering v0.15.8\u001b[39m\n",
      "  \u001b[90m[944b1d66] \u001b[39m\u001b[92m+ CodecZlib v0.7.8\u001b[39m\n",
      "  \u001b[90m[35d6a980] \u001b[39m\u001b[92m+ ColorSchemes v3.28.0\u001b[39m\n",
      "  \u001b[90m[3da002f7] \u001b[39m\u001b[92m+ ColorTypes v0.12.0\u001b[39m\n",
      "  \u001b[90m[c3611d14] \u001b[39m\u001b[92m+ ColorVectorSpace v0.11.0\u001b[39m\n",
      "  \u001b[90m[5ae59095] \u001b[39m\u001b[92m+ Colors v0.13.0\u001b[39m\n",
      "  \u001b[90m[34da2185] \u001b[39m\u001b[92m+ Compat v4.16.0\u001b[39m\n",
      "  \u001b[90m[a33af91c] \u001b[39m\u001b[92m+ CompositionsBase v0.1.2\u001b[39m\n",
      "  \u001b[90m[f0e56b4a] \u001b[39m\u001b[92m+ ConcurrentUtilities v2.5.0\u001b[39m\n",
      "  \u001b[90m[d38c429a] \u001b[39m\u001b[92m+ Contour v0.6.3\u001b[39m\n",
      "  \u001b[90m[a8cc5b0e] \u001b[39m\u001b[92m+ Crayons v4.1.1\u001b[39m\n",
      "  \u001b[90m[9a962f9c] \u001b[39m\u001b[92m+ DataAPI v1.16.0\u001b[39m\n",
      "  \u001b[90m[a93c6f00] \u001b[39m\u001b[92m+ DataFrames v1.7.0\u001b[39m\n",
      "  \u001b[90m[864edb3b] \u001b[39m\u001b[92m+ DataStructures v0.18.20\u001b[39m\n",
      "  \u001b[90m[e2d170a0] \u001b[39m\u001b[92m+ DataValueInterfaces v1.0.0\u001b[39m\n",
      "  \u001b[90m[8bb1440f] \u001b[39m\u001b[92m+ DelimitedFiles v1.9.1\u001b[39m\n",
      "  \u001b[90m[b4f34e82] \u001b[39m\u001b[92m+ Distances v0.10.12\u001b[39m\n",
      "  \u001b[90m[31c24e10] \u001b[39m\u001b[92m+ Distributions v0.25.117\u001b[39m\n",
      "  \u001b[90m[ffbed154] \u001b[39m\u001b[92m+ DocStringExtensions v0.9.3\u001b[39m\n",
      "  \u001b[90m[460bff9d] \u001b[39m\u001b[92m+ ExceptionUnwrapping v0.1.11\u001b[39m\n",
      "  \u001b[90m[c87230d0] \u001b[39m\u001b[92m+ FFMPEG v0.4.2\u001b[39m\n",
      "  \u001b[90m[7a1cc6ca] \u001b[39m\u001b[92m+ FFTW v1.8.1\u001b[39m\n",
      "  \u001b[90m[5789e2e9] \u001b[39m\u001b[92m+ FileIO v1.16.6\u001b[39m\n",
      "  \u001b[90m[48062228] \u001b[39m\u001b[92m+ FilePathsBase v0.9.23\u001b[39m\n",
      "  \u001b[90m[1a297f60] \u001b[39m\u001b[92m+ FillArrays v1.13.0\u001b[39m\n",
      "  \u001b[90m[53c48c17] \u001b[39m\u001b[92m+ FixedPointNumbers v0.8.5\u001b[39m\n",
      "  \u001b[90m[1fa38f19] \u001b[39m\u001b[92m+ Format v1.3.7\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[d9f16b24] \u001b[39m\u001b[92m+ Functors v0.4.12\u001b[39m\n",
      "  \u001b[90m[28b8d3ca] \u001b[39m\u001b[92m+ GR v0.73.12\u001b[39m\n",
      "  \u001b[90m[42e2da0e] \u001b[39m\u001b[92m+ Grisu v1.0.2\u001b[39m\n",
      "  \u001b[90m[cd3eb016] \u001b[39m\u001b[92m+ HTTP v1.10.15\u001b[39m\n",
      "  \u001b[90m[34004b35] \u001b[39m\u001b[92m+ HypergeometricFunctions v0.3.27\u001b[39m\n",
      "  \u001b[90m[842dd82b] \u001b[39m\u001b[92m+ InlineStrings v1.4.2\u001b[39m\n",
      "  \u001b[90m[a98d9a8b] \u001b[39m\u001b[92m+ Interpolations v0.15.1\u001b[39m\n",
      "  \u001b[90m[41ab1584] \u001b[39m\u001b[92m+ InvertedIndices v1.3.1\u001b[39m\n",
      "  \u001b[90m[92d709cd] \u001b[39m\u001b[92m+ IrrationalConstants v0.2.4\u001b[39m\n",
      "  \u001b[90m[82899510] \u001b[39m\u001b[92m+ IteratorInterfaceExtensions v1.0.0\u001b[39m\n",
      "  \u001b[90m[033835bb] \u001b[39m\u001b[92m+ JLD2 v0.5.11\u001b[39m\n",
      "  \u001b[90m[1019f520] \u001b[39m\u001b[92m+ JLFzf v0.1.9\u001b[39m\n",
      "  \u001b[90m[692b3bcd] \u001b[39m\u001b[92m+ JLLWrappers v1.7.0\u001b[39m\n",
      "  \u001b[90m[682c06a0] \u001b[39m\u001b[92m+ JSON v0.21.4\u001b[39m\n",
      "  \u001b[90m[5ab0869b] \u001b[39m\u001b[92m+ KernelDensity v0.6.9\u001b[39m\n",
      "  \u001b[90m[ec8451be] \u001b[39m\u001b[92m+ KernelFunctions v0.10.64\u001b[39m\n",
      "  \u001b[90m[b964fa9f] \u001b[39m\u001b[92m+ LaTeXStrings v1.4.0\u001b[39m\n",
      "  \u001b[90m[23fbe1c1] \u001b[39m\u001b[92m+ Latexify v0.16.6\u001b[39m\n",
      "  \u001b[90m[2ab3a3ac] \u001b[39m\u001b[92m+ LogExpFunctions v0.3.29\u001b[39m\n",
      "  \u001b[90m[e6f89c97] \u001b[39m\u001b[92m+ LoggingExtras v1.1.0\u001b[39m\n",
      "  \u001b[90m[1914dd2f] \u001b[39m\u001b[92m+ MacroTools v0.5.15\u001b[39m\n",
      "  \u001b[90m[739be429] \u001b[39m\u001b[92m+ MbedTLS v1.1.9\u001b[39m\n",
      "  \u001b[90m[442fdcdd] \u001b[39m\u001b[92m+ Measures v0.3.2\u001b[39m\n",
      "  \u001b[90m[e1d29d7a] \u001b[39m\u001b[92m+ Missings v1.2.0\u001b[39m\n",
      "  \u001b[90m[6f286f6a] \u001b[39m\u001b[92m+ MultivariateStats v0.10.3\u001b[39m\n",
      "  \u001b[90m[77ba4419] \u001b[39m\u001b[92m+ NaNMath v1.1.2\u001b[39m\n",
      "  \u001b[90m[b8a86587] \u001b[39m\u001b[92m+ NearestNeighbors v0.4.21\u001b[39m\n",
      "  \u001b[90m[510215fc] \u001b[39m\u001b[92m+ Observables v0.5.5\u001b[39m\n",
      "  \u001b[90m[6fe1bfb0] \u001b[39m\u001b[92m+ OffsetArrays v1.15.0\u001b[39m\n",
      "  \u001b[90m[4d8831e6] \u001b[39m\u001b[92m+ OpenSSL v1.4.3\u001b[39m\n",
      "  \u001b[90m[bac558e1] \u001b[39m\u001b[92m+ OrderedCollections v1.8.0\u001b[39m\n",
      "  \u001b[90m[90014a1f] \u001b[39m\u001b[92m+ PDMats v0.11.32\u001b[39m\n",
      "  \u001b[90m[69de0a69] \u001b[39m\u001b[92m+ Parsers v2.8.1\u001b[39m\n",
      "  \u001b[90m[b98c9c47] \u001b[39m\u001b[92m+ Pipe v1.3.0\u001b[39m\n",
      "  \u001b[90m[ccf2f8ad] \u001b[39m\u001b[92m+ PlotThemes v3.3.0\u001b[39m\n",
      "  \u001b[90m[995b91a9] \u001b[39m\u001b[92m+ PlotUtils v1.4.3\u001b[39m\n",
      "  \u001b[90m[91a5bcdd] \u001b[39m\u001b[92m+ Plots v1.40.9\u001b[39m\n",
      "  \u001b[90m[2dfb63ee] \u001b[39m\u001b[92m+ PooledArrays v1.4.3\u001b[39m\n",
      "  \u001b[90m[aea7be01] \u001b[39m\u001b[92m+ PrecompileTools v1.2.1\u001b[39m\n",
      "  \u001b[90m[21216c6a] \u001b[39m\u001b[92m+ Preferences v1.4.3\u001b[39m\n",
      "  \u001b[90m[08abe8d2] \u001b[39m\u001b[92m+ PrettyTables v2.4.0\u001b[39m\n",
      "  \u001b[90m[43287f4e] \u001b[39m\u001b[92m+ PtrArrays v1.3.0\u001b[39m\n",
      "  \u001b[90m[1fd47b50] \u001b[39m\u001b[92m+ QuadGK v2.11.2\u001b[39m\n",
      "  \u001b[90m[c84ed2f1] \u001b[39m\u001b[92m+ Ratios v0.4.5\u001b[39m\n",
      "  \u001b[90m[3cdcf5f2] \u001b[39m\u001b[92m+ RecipesBase v1.3.4\u001b[39m\n",
      "  \u001b[90m[01d81517] \u001b[39m\u001b[92m+ RecipesPipeline v0.6.12\u001b[39m\n",
      "  \u001b[90m[189a3867] \u001b[39m\u001b[92m+ Reexport v1.2.2\u001b[39m\n",
      "  \u001b[90m[05181044] \u001b[39m\u001b[92m+ RelocatableFolders v1.0.1\u001b[39m\n",
      "  \u001b[90m[ae029012] \u001b[39m\u001b[92m+ Requires v1.3.0\u001b[39m\n",
      "  \u001b[90m[79098fc4] \u001b[39m\u001b[92m+ Rmath v0.8.0\u001b[39m\n",
      "  \u001b[90m[6c6a2e73] \u001b[39m\u001b[92m+ Scratch v1.2.1\u001b[39m\n",
      "  \u001b[90m[91c51154] \u001b[39m\u001b[92m+ SentinelArrays v1.4.8\u001b[39m\n",
      "  \u001b[90m[992d4aef] \u001b[39m\u001b[92m+ Showoff v1.0.3\u001b[39m\n",
      "  \u001b[90m[777ac1f9] \u001b[39m\u001b[92m+ SimpleBufferStream v1.2.0\u001b[39m\n",
      "  \u001b[90m[a2af1166] \u001b[39m\u001b[92m+ SortingAlgorithms v1.2.1\u001b[39m\n",
      "  \u001b[90m[276daf66] \u001b[39m\u001b[92m+ SpecialFunctions v2.5.0\u001b[39m\n",
      "  \u001b[90m[860ef19b] \u001b[39m\u001b[92m+ StableRNGs v1.0.2\u001b[39m\n",
      "  \u001b[90m[90137ffa] \u001b[39m\u001b[92m+ StaticArrays v1.9.12\u001b[39m\n",
      "  \u001b[90m[1e83bf80] \u001b[39m\u001b[92m+ StaticArraysCore v1.4.3\u001b[39m\n",
      "  \u001b[90m[10745b16] \u001b[39m\u001b[92m+ Statistics v1.11.1\u001b[39m\n",
      "  \u001b[90m[82ae8749] \u001b[39m\u001b[92m+ StatsAPI v1.7.0\u001b[39m\n",
      "  \u001b[90m[2913bbd2] \u001b[39m\u001b[92m+ StatsBase v0.34.4\u001b[39m\n",
      "  \u001b[90m[4c63d2b9] \u001b[39m\u001b[92m+ StatsFuns v1.3.2\u001b[39m\n",
      "  \u001b[90m[f3b207a7] \u001b[39m\u001b[92m+ StatsPlots v0.15.7\u001b[39m\n",
      "  \u001b[90m[892a3eda] \u001b[39m\u001b[92m+ StringManipulation v0.4.1\u001b[39m\n",
      "  \u001b[90m[ab02a1b2] \u001b[39m\u001b[92m+ TableOperations v1.2.0\u001b[39m\n",
      "  \u001b[90m[3783bdb8] \u001b[39m\u001b[92m+ TableTraits v1.0.1\u001b[39m\n",
      "  \u001b[90m[bd369af6] \u001b[39m\u001b[92m+ Tables v1.12.0\u001b[39m\n",
      "  \u001b[90m[62fd8b95] \u001b[39m\u001b[92m+ TensorCore v0.1.1\u001b[39m\n",
      "  \u001b[90m[3bb67fe8] \u001b[39m\u001b[92m+ TranscodingStreams v0.11.3\u001b[39m\n",
      "  \u001b[90m[5c2747f8] \u001b[39m\u001b[92m+ URIs v1.5.1\u001b[39m\n",
      "  \u001b[90m[1cfade01] \u001b[39m\u001b[92m+ UnicodeFun v0.4.1\u001b[39m\n",
      "  \u001b[90m[1986cc42] \u001b[39m\u001b[92m+ Unitful v1.22.0\u001b[39m\n",
      "  \u001b[90m[45397f5d] \u001b[39m\u001b[92m+ UnitfulLatexify v1.6.4\u001b[39m\n",
      "  \u001b[90m[41fe7b60] \u001b[39m\u001b[92m+ Unzip v0.2.0\u001b[39m\n",
      "  \u001b[90m[ea10d353] \u001b[39m\u001b[92m+ WeakRefStrings v1.4.2\u001b[39m\n",
      "  \u001b[90m[cc8bc4a8] \u001b[39m\u001b[92m+ Widgets v0.6.7\u001b[39m\n",
      "  \u001b[90m[efce3f68] \u001b[39m\u001b[92m+ WoodburyMatrices v1.0.0\u001b[39m\n",
      "  \u001b[90m[76eceee3] \u001b[39m\u001b[92m+ WorkerUtilities v1.6.1\u001b[39m\n",
      "  \u001b[90m[700de1a5] \u001b[39m\u001b[92m+ ZygoteRules v0.2.7\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[68821587] \u001b[39m\u001b[92m+ Arpack_jll v3.5.1+1\u001b[39m\n",
      "  \u001b[90m[6e34b625] \u001b[39m\u001b[92m+ Bzip2_jll v1.0.9+0\u001b[39m\n",
      "  \u001b[90m[83423d85] \u001b[39m\u001b[92m+ Cairo_jll v1.18.2+1\u001b[39m\n",
      "  \u001b[90m[ee1fde0b] \u001b[39m\u001b[92m+ Dbus_jll v1.14.10+0\u001b[39m\n",
      "  \u001b[90m[2702e6a9] \u001b[39m\u001b[92m+ EpollShim_jll v0.0.20230411+1\u001b[39m\n",
      "  \u001b[90m[2e619515] \u001b[39m\u001b[92m+ Expat_jll v2.6.5+0\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[b22a6f82] \u001b[39m\u001b[92m+ FFMPEG_jll v4.4.4+1\u001b[39m\n",
      "  \u001b[90m[f5851436] \u001b[39m\u001b[92m+ FFTW_jll v3.3.10+3\u001b[39m\n",
      "  \u001b[90m[a3f928ae] \u001b[39m\u001b[92m+ Fontconfig_jll v2.15.0+0\u001b[39m\n",
      "  \u001b[90m[d7e528f0] \u001b[39m\u001b[92m+ FreeType2_jll v2.13.3+1\u001b[39m\n",
      "  \u001b[90m[559328eb] \u001b[39m\u001b[92m+ FriBidi_jll v1.0.16+0\u001b[39m\n",
      "  \u001b[90m[0656b61e] \u001b[39m\u001b[92m+ GLFW_jll v3.4.0+2\u001b[39m\n",
      "  \u001b[90m[d2c73de3] \u001b[39m\u001b[92m+ GR_jll v0.73.12+0\u001b[39m\n",
      "  \u001b[90m[78b55507] \u001b[39m\u001b[92m+ Gettext_jll v0.21.0+0\u001b[39m\n",
      "  \u001b[90m[7746bdde] \u001b[39m\u001b[92m+ Glib_jll v2.82.4+0\u001b[39m\n",
      "  \u001b[90m[3b182d85] \u001b[39m\u001b[92m+ Graphite2_jll v1.3.14+1\u001b[39m\n",
      "  \u001b[90m[2e76f6c2] \u001b[39m\u001b[92m+ HarfBuzz_jll v8.5.0+0\u001b[39m\n",
      "  \u001b[90m[1d5cc7b8] \u001b[39m\u001b[92m+ IntelOpenMP_jll v2025.0.4+0\u001b[39m\n",
      "  \u001b[90m[aacddb02] \u001b[39m\u001b[92m+ JpegTurbo_jll v3.1.1+0\u001b[39m\n",
      "  \u001b[90m[c1c5ebd0] \u001b[39m\u001b[92m+ LAME_jll v3.100.2+0\u001b[39m\n",
      "  \u001b[90m[88015f11] \u001b[39m\u001b[92m+ LERC_jll v4.0.1+0\u001b[39m\n",
      "  \u001b[90m[1d63c593] \u001b[39m\u001b[92m+ LLVMOpenMP_jll v18.1.7+0\u001b[39m\n",
      "  \u001b[90m[dd4b983a] \u001b[39m\u001b[92m+ LZO_jll v2.10.3+0\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[e9f186c6] \u001b[39m\u001b[92m+ Libffi_jll v3.2.2+2\u001b[39m\n",
      "  \u001b[90m[d4300ac3] \u001b[39m\u001b[92m+ Libgcrypt_jll v1.11.0+0\u001b[39m\n",
      "  \u001b[90m[7e76a0d4] \u001b[39m\u001b[92m+ Libglvnd_jll v1.7.0+0\u001b[39m\n",
      "  \u001b[90m[7add5ba3] \u001b[39m\u001b[92m+ Libgpg_error_jll v1.51.1+0\u001b[39m\n",
      "  \u001b[90m[94ce4f54] \u001b[39m\u001b[92m+ Libiconv_jll v1.18.0+0\u001b[39m\n",
      "  \u001b[90m[4b2f31a3] \u001b[39m\u001b[92m+ Libmount_jll v2.40.3+0\u001b[39m\n",
      "  \u001b[90m[89763e89] \u001b[39m\u001b[92m+ Libtiff_jll v4.7.1+0\u001b[39m\n",
      "  \u001b[90m[38a345b3] \u001b[39m\u001b[92m+ Libuuid_jll v2.40.3+0\u001b[39m\n",
      "  \u001b[90m[856f044c] \u001b[39m\u001b[92m+ MKL_jll v2025.0.1+1\u001b[39m\n",
      "  \u001b[90m[e7412a2a] \u001b[39m\u001b[92m+ Ogg_jll v1.3.5+1\u001b[39m\n",
      "  \u001b[90m[458c3c95] \u001b[39m\u001b[92m+ OpenSSL_jll v3.0.16+0\u001b[39m\n",
      "  \u001b[90m[efe28fd5] \u001b[39m\u001b[92m+ OpenSpecFun_jll v0.5.6+0\u001b[39m\n",
      "  \u001b[90m[91d4177d] \u001b[39m\u001b[92m+ Opus_jll v1.3.3+0\u001b[39m\n",
      "  \u001b[90m[36c8627f] \u001b[39m\u001b[92m+ Pango_jll v1.55.5+0\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[30392449] \u001b[39m\u001b[92m+ Pixman_jll v0.43.4+0\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[c0090381] \u001b[39m\u001b[92m+ Qt6Base_jll v6.7.1+1\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[629bc702] \u001b[39m\u001b[92m+ Qt6Declarative_jll v6.7.1+2\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[ce943373] \u001b[39m\u001b[92m+ Qt6ShaderTools_jll v6.7.1+1\u001b[39m\n",
      "\u001b[32mâŒƒ\u001b[39m \u001b[90m[e99dba38] \u001b[39m\u001b[92m+ Qt6Wayland_jll v6.7.1+1\u001b[39m\n",
      "  \u001b[90m[f50d1b31] \u001b[39m\u001b[92m+ Rmath_jll v0.5.1+0\u001b[39m\n",
      "  \u001b[90m[a44049a8] \u001b[39m\u001b[92m+ Vulkan_Loader_jll v1.3.243+0\u001b[39m\n",
      "  \u001b[90m[a2964d1f] \u001b[39m\u001b[92m+ Wayland_jll v1.21.0+2\u001b[39m\n",
      "  \u001b[90m[2381bf8a] \u001b[39m\u001b[92m+ Wayland_protocols_jll v1.36.0+0\u001b[39m\n",
      "  \u001b[90m[02c8fc9c] \u001b[39m\u001b[92m+ XML2_jll v2.13.6+0\u001b[39m\n",
      "  \u001b[90m[aed1982a] \u001b[39m\u001b[92m+ XSLT_jll v1.1.42+0\u001b[39m\n",
      "  \u001b[90m[ffd25f8a] \u001b[39m\u001b[92m+ XZ_jll v5.6.4+1\u001b[39m\n",
      "  \u001b[90m[f67eecfb] \u001b[39m\u001b[92m+ Xorg_libICE_jll v1.1.1+0\u001b[39m\n",
      "  \u001b[90m[c834827a] \u001b[39m\u001b[92m+ Xorg_libSM_jll v1.2.4+0\u001b[39m\n",
      "  \u001b[90m[4f6342f7] \u001b[39m\u001b[92m+ Xorg_libX11_jll v1.8.6+3\u001b[39m\n",
      "  \u001b[90m[0c0b7dd1] \u001b[39m\u001b[92m+ Xorg_libXau_jll v1.0.12+0\u001b[39m\n",
      "  \u001b[90m[935fb764] \u001b[39m\u001b[92m+ Xorg_libXcursor_jll v1.2.3+0\u001b[39m\n",
      "  \u001b[90m[a3789734] \u001b[39m\u001b[92m+ Xorg_libXdmcp_jll v1.1.5+0\u001b[39m\n",
      "  \u001b[90m[1082639a] \u001b[39m\u001b[92m+ Xorg_libXext_jll v1.3.6+3\u001b[39m\n",
      "  \u001b[90m[d091e8ba] \u001b[39m\u001b[92m+ Xorg_libXfixes_jll v6.0.0+0\u001b[39m\n",
      "  \u001b[90m[a51aa0fd] \u001b[39m\u001b[92m+ Xorg_libXi_jll v1.8.2+0\u001b[39m\n",
      "  \u001b[90m[d1454406] \u001b[39m\u001b[92m+ Xorg_libXinerama_jll v1.1.5+0\u001b[39m\n",
      "  \u001b[90m[ec84b674] \u001b[39m\u001b[92m+ Xorg_libXrandr_jll v1.5.4+0\u001b[39m\n",
      "  \u001b[90m[ea2f1a96] \u001b[39m\u001b[92m+ Xorg_libXrender_jll v0.9.11+1\u001b[39m\n",
      "  \u001b[90m[14d82f49] \u001b[39m\u001b[92m+ Xorg_libpthread_stubs_jll v0.1.2+0\u001b[39m\n",
      "  \u001b[90m[c7cfdc94] \u001b[39m\u001b[92m+ Xorg_libxcb_jll v1.17.0+3\u001b[39m\n",
      "  \u001b[90m[cc61e674] \u001b[39m\u001b[92m+ Xorg_libxkbfile_jll v1.1.2+1\u001b[39m\n",
      "  \u001b[90m[e920d4aa] \u001b[39m\u001b[92m+ Xorg_xcb_util_cursor_jll v0.1.4+0\u001b[39m\n",
      "  \u001b[90m[12413925] \u001b[39m\u001b[92m+ Xorg_xcb_util_image_jll v0.4.0+1\u001b[39m\n",
      "  \u001b[90m[2def613f] \u001b[39m\u001b[92m+ Xorg_xcb_util_jll v0.4.0+1\u001b[39m\n",
      "  \u001b[90m[975044d2] \u001b[39m\u001b[92m+ Xorg_xcb_util_keysyms_jll v0.4.0+1\u001b[39m\n",
      "  \u001b[90m[0d47668e] \u001b[39m\u001b[92m+ Xorg_xcb_util_renderutil_jll v0.3.9+1\u001b[39m\n",
      "  \u001b[90m[c22f9ab0] \u001b[39m\u001b[92m+ Xorg_xcb_util_wm_jll v0.4.1+1\u001b[39m\n",
      "  \u001b[90m[35661453] \u001b[39m\u001b[92m+ Xorg_xkbcomp_jll v1.4.6+1\u001b[39m\n",
      "  \u001b[90m[33bec58e] \u001b[39m\u001b[92m+ Xorg_xkeyboard_config_jll v2.39.0+0\u001b[39m\n",
      "  \u001b[90m[c5fb5394] \u001b[39m\u001b[92m+ Xorg_xtrans_jll v1.5.1+0\u001b[39m\n",
      "  \u001b[90m[3161d3a3] \u001b[39m\u001b[92m+ Zstd_jll v1.5.7+0\u001b[39m\n",
      "  \u001b[90m[35ca27e7] \u001b[39m\u001b[92m+ eudev_jll v3.2.9+0\u001b[39m\n",
      "  \u001b[90m[214eeab7] \u001b[39m\u001b[92m+ fzf_jll v0.56.3+0\u001b[39m\n",
      "  \u001b[90m[1a1c6b14] \u001b[39m\u001b[92m+ gperf_jll v3.1.1+1\u001b[39m\n",
      "  \u001b[90m[a4ae2306] \u001b[39m\u001b[92m+ libaom_jll v3.11.0+0\u001b[39m\n",
      "  \u001b[90m[0ac62f75] \u001b[39m\u001b[92m+ libass_jll v0.15.2+0\u001b[39m\n",
      "  \u001b[90m[1183f4f0] \u001b[39m\u001b[92m+ libdecor_jll v0.2.2+0\u001b[39m\n",
      "  \u001b[90m[2db6ffa8] \u001b[39m\u001b[92m+ libevdev_jll v1.11.0+0\u001b[39m\n",
      "  \u001b[90m[f638f0a6] \u001b[39m\u001b[92m+ libfdk_aac_jll v2.0.3+0\u001b[39m\n",
      "  \u001b[90m[36db933b] \u001b[39m\u001b[92m+ libinput_jll v1.18.0+0\u001b[39m\n",
      "  \u001b[90m[b53b4c65] \u001b[39m\u001b[92m+ libpng_jll v1.6.46+0\u001b[39m\n",
      "  \u001b[90m[f27f6e37] \u001b[39m\u001b[92m+ libvorbis_jll v1.3.7+2\u001b[39m\n",
      "  \u001b[90m[009596ad] \u001b[39m\u001b[92m+ mtdev_jll v1.1.6+0\u001b[39m\n",
      "  \u001b[90m[1317d2d5] \u001b[39m\u001b[92m+ oneTBB_jll v2021.12.0+0\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[1270edf5] \u001b[39m\u001b[92m+ x264_jll v2021.5.5+0\u001b[39m\n",
      "\u001b[33mâŒ…\u001b[39m \u001b[90m[dfaa095f] \u001b[39m\u001b[92m+ x265_jll v3.5.0+0\u001b[39m\n",
      "  \u001b[90m[d8fb68d0] \u001b[39m\u001b[92m+ xkbcommon_jll v1.4.1+2\u001b[39m\n",
      "  \u001b[90m[0dad84c5] \u001b[39m\u001b[92m+ ArgTools v1.1.2\u001b[39m\n",
      "  \u001b[90m[56f22d72] \u001b[39m\u001b[92m+ Artifacts v1.11.0\u001b[39m\n",
      "  \u001b[90m[2a0f44e3] \u001b[39m\u001b[92m+ Base64 v1.11.0\u001b[39m\n",
      "  \u001b[90m[ade2ca70] \u001b[39m\u001b[92m+ Dates v1.11.0\u001b[39m\n",
      "  \u001b[90m[8ba89e20] \u001b[39m\u001b[92m+ Distributed v1.11.0\u001b[39m\n",
      "  \u001b[90m[f43a241f] \u001b[39m\u001b[92m+ Downloads v1.6.0\u001b[39m\n",
      "  \u001b[90m[7b1f6079] \u001b[39m\u001b[92m+ FileWatching v1.11.0\u001b[39m\n",
      "  \u001b[90m[9fa8497b] \u001b[39m\u001b[92m+ Future v1.11.0\u001b[39m\n",
      "  \u001b[90m[b77e0a4c] \u001b[39m\u001b[92m+ InteractiveUtils v1.11.0\u001b[39m\n",
      "  \u001b[90m[4af54fe1] \u001b[39m\u001b[92m+ LazyArtifacts v1.11.0\u001b[39m\n",
      "  \u001b[90m[b27032c2] \u001b[39m\u001b[92m+ LibCURL v0.6.4\u001b[39m\n",
      "  \u001b[90m[76f85450] \u001b[39m\u001b[92m+ LibGit2 v1.11.0\u001b[39m\n",
      "  \u001b[90m[8f399da3] \u001b[39m\u001b[92m+ Libdl v1.11.0\u001b[39m\n",
      "  \u001b[90m[37e2e46d] \u001b[39m\u001b[92m+ LinearAlgebra v1.11.0\u001b[39m\n",
      "  \u001b[90m[56ddb016] \u001b[39m\u001b[92m+ Logging v1.11.0\u001b[39m\n",
      "  \u001b[90m[d6f4376e] \u001b[39m\u001b[92m+ Markdown v1.11.0\u001b[39m\n",
      "  \u001b[90m[a63ad114] \u001b[39m\u001b[92m+ Mmap v1.11.0\u001b[39m\n",
      "  \u001b[90m[ca575930] \u001b[39m\u001b[92m+ NetworkOptions v1.2.0\u001b[39m\n",
      "  \u001b[90m[44cfe95a] \u001b[39m\u001b[92m+ Pkg v1.11.0\u001b[39m\n",
      "  \u001b[90m[de0858da] \u001b[39m\u001b[92m+ Printf v1.11.0\u001b[39m\n",
      "  \u001b[90m[3fa0cd96] \u001b[39m\u001b[92m+ REPL v1.11.0\u001b[39m\n",
      "  \u001b[90m[9a3f8284] \u001b[39m\u001b[92m+ Random v1.11.0\u001b[39m\n",
      "  \u001b[90m[ea8e919c] \u001b[39m\u001b[92m+ SHA v0.7.0\u001b[39m\n",
      "  \u001b[90m[9e88b42a] \u001b[39m\u001b[92m+ Serialization v1.11.0\u001b[39m\n",
      "  \u001b[90m[1a1011a3] \u001b[39m\u001b[92m+ SharedArrays v1.11.0\u001b[39m\n",
      "  \u001b[90m[6462fe0b] \u001b[39m\u001b[92m+ Sockets v1.11.0\u001b[39m\n",
      "  \u001b[90m[2f01184e] \u001b[39m\u001b[92m+ SparseArrays v1.11.0\u001b[39m\n",
      "  \u001b[90m[f489334b] \u001b[39m\u001b[92m+ StyledStrings v1.11.0\u001b[39m\n",
      "  \u001b[90m[4607b0f0] \u001b[39m\u001b[92m+ SuiteSparse\u001b[39m\n",
      "  \u001b[90m[fa267f1f] \u001b[39m\u001b[92m+ TOML v1.0.3\u001b[39m\n",
      "  \u001b[90m[a4e569a6] \u001b[39m\u001b[92m+ Tar v1.10.0\u001b[39m\n",
      "  \u001b[90m[8dfed614] \u001b[39m\u001b[92m+ Test v1.11.0\u001b[39m\n",
      "  \u001b[90m[cf7118a7] \u001b[39m\u001b[92m+ UUIDs v1.11.0\u001b[39m\n",
      "  \u001b[90m[4ec0a83e] \u001b[39m\u001b[92m+ Unicode v1.11.0\u001b[39m\n",
      "  \u001b[90m[e66e0078] \u001b[39m\u001b[92m+ CompilerSupportLibraries_jll v1.1.1+0\u001b[39m\n",
      "  \u001b[90m[deac9b47] \u001b[39m\u001b[92m+ LibCURL_jll v8.6.0+0\u001b[39m\n",
      "  \u001b[90m[e37daf67] \u001b[39m\u001b[92m+ LibGit2_jll v1.7.2+0\u001b[39m\n",
      "  \u001b[90m[29816b5a] \u001b[39m\u001b[92m+ LibSSH2_jll v1.11.0+1\u001b[39m\n",
      "  \u001b[90m[c8ffd9c3] \u001b[39m\u001b[92m+ MbedTLS_jll v2.28.6+0\u001b[39m\n",
      "  \u001b[90m[14a3606d] \u001b[39m\u001b[92m+ MozillaCACerts_jll v2023.12.12\u001b[39m\n",
      "  \u001b[90m[4536629a] \u001b[39m\u001b[92m+ OpenBLAS_jll v0.3.27+1\u001b[39m\n",
      "  \u001b[90m[05823500] \u001b[39m\u001b[92m+ OpenLibm_jll v0.8.1+2\u001b[39m\n",
      "  \u001b[90m[efcefdf7] \u001b[39m\u001b[92m+ PCRE2_jll v10.42.0+1\u001b[39m\n",
      "  \u001b[90m[bea87d4a] \u001b[39m\u001b[92m+ SuiteSparse_jll v7.7.0+0\u001b[39m\n",
      "  \u001b[90m[83775a58] \u001b[39m\u001b[92m+ Zlib_jll v1.2.13+1\u001b[39m\n",
      "  \u001b[90m[8e850b90] \u001b[39m\u001b[92m+ libblastrampoline_jll v5.11.0+0\u001b[39m\n",
      "  \u001b[90m[8e850ede] \u001b[39m\u001b[92m+ nghttp2_jll v1.59.0+0\u001b[39m\n",
      "  \u001b[90m[3f19e933] \u001b[39m\u001b[92m+ p7zip_jll v17.4.0+2\u001b[39m\n",
      "\u001b[36m\u001b[1m        Info\u001b[22m\u001b[39m Packages marked with \u001b[32mâŒƒ\u001b[39m and \u001b[33mâŒ…\u001b[39m have new versions available. Those with \u001b[32mâŒƒ\u001b[39m may be upgradable, but those with \u001b[33mâŒ…\u001b[39m are restricted by compatibility constraints from upgrading. To see why use `status --outdated -m`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a/Manifest.toml`\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: Package Colors not found in current path, maybe you meant `import/using .Colors`.\n- Otherwise, run `import Pkg; Pkg.add(\"Colors\")` to install the Colors package.\nin expression starting at /Users/alinachisti/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a/Include.jl:16",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: Package Colors not found in current path, maybe you meant `import/using .Colors`.\n- Otherwise, run `import Pkg; Pkg.add(\"Colors\")` to install the Colors package.\nin expression starting at /Users/alinachisti/Documents/CHEME-5820-Labs-Spring-2025/labs/week-4/L4a/Include.jl:16",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ ./loading.jl:2296 [inlined]",
      " [2] macro expansion",
      "   @ ./lock.jl:273 [inlined]",
      " [3] __require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:2271",
      " [4] #invoke_in_world#3",
      "   @ ./essentials.jl:1089 [inlined]",
      " [5] invoke_in_world",
      "   @ ./essentials.jl:1086 [inlined]",
      " [6] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:2260",
      " [7] include(fname::String)",
      "   @ Main ./sysimg.jl:38",
      " [8] top-level scope",
      "   @ In[1]:1"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c49399-30b1-415c-af31-c4e5d581bff3",
   "metadata": {},
   "source": [
    "### Data\n",
    "We gathered a daily open-high-low-close `dataset` for each firm in the [S&P500](https://en.wikipedia.org/wiki/S%26P_500) from `01-03-2014` until `02-07-2025`, along with data for a few exchange-traded funds and volatility products during that time. We load the `orignal_dataset` by calling the `MyMarketDataSet()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebfce991-691c-4440-a162-0a31477a53f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `MyMarketDataSet` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `MyMarketDataSet` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[2]:1"
     ]
    }
   ],
   "source": [
    "original_dataset = MyMarketDataSet() |> x-> x[\"dataset\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0879f0-b963-4762-a4a4-500b78ff23c7",
   "metadata": {},
   "source": [
    "\n",
    "__Clean the data__: Not all tickers in our dataset have the maximum number of trading days for various reasons, e.g., acquisition or de-listing events. Let's collect only those tickers with the maximum number of trading days.\n",
    "\n",
    "* First, let's compute the number of records for a company that we know has a maximum value, e.g., `AAPL`, and save that value in the `maximum_number_trading_days` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07fe044c-6ac4-4381-9954-53f131213758",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `original_dataset` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `original_dataset` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:1"
     ]
    }
   ],
   "source": [
    "maximum_number_trading_days = original_dataset[\"AAPL\"] |> nrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7db52a-90f3-4eb4-b333-187c0e516580",
   "metadata": {},
   "source": [
    "Now, lets iterate through our data and collect only those tickers that have `maximum_number_trading_days` records. Save that data in the `dataset::Dict{String,DataFrame}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2faa1d52-8169-4ebf-b3fd-6ac10c6a4a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `DataFrame` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `DataFrame` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[4]:3"
     ]
    }
   ],
   "source": [
    "dataset = let\n",
    "\n",
    "    dataset = Dict{String,DataFrame}();\n",
    "    for (ticker,data) âˆˆ original_dataset\n",
    "        if (nrow(data) == maximum_number_trading_days)\n",
    "            dataset[ticker] = data;\n",
    "        end\n",
    "    end\n",
    "    dataset\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f531410-ac51-4252-a52a-a2d9ca70bdd1",
   "metadata": {},
   "source": [
    "Let's get a list of firms in the cleaned up `dataset` and save it in the `all_tickers` array. We sort the firms alphabetically from `A` to `Z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a704c6-efca-45af-ac33-5ec4f929f3c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `dataset` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `dataset` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[5]:1"
     ]
    }
   ],
   "source": [
    "list_of_all_tickers = keys(dataset) |> collect |> sort;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a1ddf-7816-4650-895e-48e25dfdeb31",
   "metadata": {},
   "source": [
    "Compute the expected (annualized) excess log growth rate by passing the `dataset` and the entire list of firms we have in the dataset to the [log_growth_matrix(...) method](src/Compute.jl). The log growth rate between time period $j-1$ to $j$, e.g., yesterday to today is defined as:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mu_{j,j-1} = \\left(\\frac{1}{\\Delta{t}}\\right)\\ln\\left(\\frac{S_{j}}{S_{j-1}}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\Delta{t}$ denotes the period time step, and $S_{j}$ denote share price in period $j$.\n",
    "* The log growth rates are stored in the `D::Array{Float64,2}` variable, a $T-1\\times{N}$ array of log return values. Each row of the `D` matrix corresponds to a time value, while each column corresponds to a firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1184c4b9-b5f8-41df-abb2-4aa62a25ccc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `log_growth_matrix` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `log_growth_matrix` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[6]:8"
     ]
    }
   ],
   "source": [
    "D = let\n",
    "\n",
    "    # setup some constants -\n",
    "    Î”t = (1/252); # 1-trading day in units of years\n",
    "    risk_free_rate = 0.0415; # inferred cc risk-free rate\n",
    "\n",
    "    # compute\n",
    "    Î¼ = log_growth_matrix(dataset, list_of_all_tickers, Î”t = Î”t, \n",
    "        risk_free_rate = risk_free_rate);\n",
    "\n",
    "    # return to caller\n",
    "    Î¼\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d45aad-ea69-42ff-b262-178baea0b067",
   "metadata": {},
   "source": [
    "Next, let's [z-score center](https://en.wikipedia.org/wiki/Feature_scaling) the continous feature data. In [z-score feature scaling](https://en.wikipedia.org/wiki/Feature_scaling), we subtract off the mean of each feature and then divide by the standard deviation, i.e., $x^{\\prime} = (x - \\mu)/\\sigma$ where $x$ is the unscaled data, and $x^{\\prime}$ is the scaled data. Under this scaling regime, $x^{\\prime}\\leq{0}$ will be values that are less than or equal to the mean value $\\mu$, while $x^{\\prime}>0$ indicate values that are greater than the mean.\n",
    "\n",
    "We save the z-score centered growth data data in the `DÌ„::Array{Float64,2}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5bcbf6f-cad4-4f7d-b549-7c4ca4733da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[7]:4"
     ]
    }
   ],
   "source": [
    "DÌ„ = let\n",
    "\n",
    "    # setup -\n",
    "    number_of_examples = size(D,1);\n",
    "\n",
    "    DÌ„ = copy(D);\n",
    "    for j âˆˆ eachindex(list_of_all_tickers)\n",
    "        Î¼ = mean(D[:,j]); # compute the mean\n",
    "        Ïƒ = std(D[:,j]); # compute std\n",
    "\n",
    "        # rescale -\n",
    "        for k âˆˆ 1:number_of_examples\n",
    "            DÌ„[k,j] = (D[k,j] - Î¼)/Ïƒ;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    DÌ„\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09feebe1-6780-4140-895e-cb4c686632d4",
   "metadata": {},
   "source": [
    "Next, (randomly) split that full dataset `D` into `training` and `test` subsets. We do this randomly, where the `number_of_training_examples::Int64` variable specifies the number of training points. The `training::Array{Float64,2}` data will be used to estimate the model parameters, and `test::Array{Float64,2}` will be used for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cc76ee-922b-47fa-b606-be5bc5009611",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `D` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[8]:4"
     ]
    }
   ],
   "source": [
    "training, test = let\n",
    "\n",
    "    number_of_training_examples = 2000; # set the number to set the number of training examples\n",
    "    number_of_examples = size(D,1); # number of rows in the full dataset\n",
    "    full_index_set = range(1,stop=number_of_examples,step=1) |> collect |> Set;\n",
    "    \n",
    "    # build index sets for training and testing\n",
    "    training_index_set = Set{Int64}();\n",
    "    for i âˆˆ 1:number_of_training_examples\n",
    "        push!(training_index_set,i); # use in order\n",
    "    end\n",
    "    test_index_set = setdiff(full_index_set,training_index_set);\n",
    "\n",
    "    # build the test and train datasets -\n",
    "    training = DÌ„[training_index_set |> collect,:];\n",
    "    test = DÌ„[test_index_set |> collect,:];\n",
    "\n",
    "    # return\n",
    "    training, test\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7156654-1426-4961-9714-5cf4fc8f1faf",
   "metadata": {},
   "source": [
    "Setup colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd996eb6-6604-4f9d-bc29-918984efec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_color_dictionary = Dict{Int64,RGB}();\n",
    "my_color_dictionary[1] = colorant\"#5e4fa2\";\n",
    "my_color_dictionary[2] = colorant\"#3288bd\";\n",
    "my_color_dictionary[3] = colorant\"#66c2a5\";\n",
    "my_color_dictionary[4] = colorant\"#abdda4\";\n",
    "my_color_dictionary[5] = colorant\"#e6f598\";\n",
    "my_color_dictionary[6] = colorant\"#fee08b\";\n",
    "my_color_dictionary[7] = colorant\"#fdae61\";\n",
    "my_color_dictionary[8] = colorant\"#f46d43\";\n",
    "my_color_dictionary[9] = colorant\"#d53e4f\";\n",
    "my_color_dictionary[10] = colorant\"#9e0142\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f5035-4b8f-443a-a9b1-fbe3058d96f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b45f42c0-9d7e-4d65-9fbe-d05900217ccf",
   "metadata": {},
   "source": [
    "## Task 3: Build, train, and test a kernel regression machine\n",
    "In this task, we'll set up, train, and test a kernel regression machine. First, we'll specify a set of `basis` stocks, i.e., our features and the kernel that we'll use to predict the growth rate of an unknown ticker (output). Then, we'll estimate the expansion parameters $\\alpha$ from the `training` data. Finally, we'll run a prediction and compare the predicted versus the observed output using the `test` dataset.\n",
    "\n",
    "Let's start by specifying _up to_ `10` tickers that we'll use as our basis feature vectors. These can be any firm in the dataset. Specify the `basis` ticker symbols in the `basis_tickers::Array{String,1}` array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce647c01-b5e5-4d61-b0c2-757c3686e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_tickers = [\"SPY\", \"JNJ\", \"PFE\", \"AAPL\", \"MSFT\", \"GS\", \"WMT\", \"AMD\"]; # we are going to write are unknown ticker, in terms of these tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a2dd0-e97b-4ed3-aef5-e37e0394cba2",
   "metadata": {},
   "source": [
    "Next, to speed things up in the simulation loop below, we'll look up the indexes of all `ticker` $\\in$ `basis_tickers` and store these in the `basis_ticker_lookup::Dict{Int,Int}` dictionary that will act as a index lookup table.\n",
    "* __Why a dictionary versus an array__? The [dictionary type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict) offers $\\mathcal{O}(1)$ (constant time, i.e., fast) lookup but is expensive to build initially compared to [an array](https://docs.julialang.org/en/v1/base/arrays/#Core.Array-Tuple%7BNothing,%20Any%7D) However, [a dictionary type](https://docs.julialang.org/en/v1/base/collections/#Base.Dict) offers a few other advantages over an array such as the `0` index hack (which I like). Thus, I have a strong preference for dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb46cdf8-35b0-46a7-9d11-6dc96549b77e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `list_of_all_tickers` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `list_of_all_tickers` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[11]:7"
     ]
    }
   ],
   "source": [
    "basis_ticker_lookup = let\n",
    "\n",
    "    # initialize -\n",
    "    basis_ticker_lookup = Dict{Int,Int}()\n",
    "\n",
    "    for i âˆˆ eachindex(basis_tickers)\n",
    "        j = findfirst(s-> s == basis_tickers[i], list_of_all_tickers); \n",
    "        basis_ticker_lookup[i] = j;\n",
    "    end\n",
    "\n",
    "    basis_ticker_lookup\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff576904-8e6b-46c5-a87c-6ea9710cb92d",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "`Unhide` the code block below to see how we plotted the daily growth rate distributions for the firms in the `basis_tickers::Array{String,1}` array for the `training` data. \n",
    "* __Summary__: The training distributions will be similar across the different tickers in the `basis_tickers::Array{String,1}` because we z-score centered the data that was used to construct the `training` and `test` datasets. These data show the characteristics of a fat-tailed distribution, i.e., skinny in the center wide of the extremes, i.e., they are _not_ normally distributed (dashed-line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed0af34-d732-4100-bdd8-f47eb6798c83",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[12]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    D = training; # what data are we using?\n",
    "    number_of_training_days = size(training,1); # number of days of data in training\n",
    "    number_of_basis_tickers = length(basis_tickers);\n",
    "\n",
    "    p = plot(bg=\"gray72\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent, \n",
    "        foreground_color_grid=:black) # make an empty plot\n",
    "    for i âˆˆ 1:number_of_basis_tickers\n",
    "        j = basis_ticker_lookup[i];\n",
    "        ticker = basis_tickers[i];\n",
    "        density!(D[:,j], label=\"$(ticker)\", c=my_color_dictionary[i], lw=2)\n",
    "    end\n",
    "    current();\n",
    "\n",
    "    # build a normal -\n",
    "    i = 4;\n",
    "    d = fit_mle(Normal, D[:,i])\n",
    "    plot!(d, ls=:dash, c=:navy, label=\"Normal $(basis_tickers[i])\")\n",
    "    \n",
    "    xlabel!(\"Daily Normalized Growth Rate (AU)\", fontsize=18)\n",
    "    ylabel!(\"Probability density (AU)\", fontsize=18)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49176dc-482d-4cee-9a69-71c7877f8c74",
   "metadata": {},
   "source": [
    "`Unhide` the code block below to see how we plotted the daily growth rate time series for the firms in the `basis_tickers::Array{String,1}` array for the training data. There should be `number_of_training_days::Int64` values for each element of the `basis_tickers::Array{String,1}` array.\n",
    "* __Summary__: The growth time series lie on top of one another but extreme events are visible for various basis tickers, i.e., the spikes. The mean growth rate (because we scaled the data) is around zero, but volatility clustering is still visible in the data, i.e., periods of high volatility are more likely to be followed by high-volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b61a19-51ea-4a58-827c-b942e87c8ed8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[13]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "\n",
    "    D = training; # what data are we using?\n",
    "    number_of_training_days = size(training,1); # number of days of data in training\n",
    "    number_of_basis_tickers = length(basis_tickers);\n",
    "\n",
    "    p = plot(bg=\"gray72\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    for i âˆˆ 1:number_of_basis_tickers\n",
    "        j = basis_ticker_lookup[i];\n",
    "        ticker = basis_tickers[i];\n",
    "        plot!(D[:,j], label=\"$(ticker)\", c=my_color_dictionary[i])\n",
    "    end\n",
    "    current();\n",
    "\n",
    "    xlabel!(\"Trading day index (AU)\", fontsize=18)\n",
    "    ylabel!(\"Daily Normalized Growth Rate (AU)\", fontsize=18)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b2082-b50e-4952-aa5a-5a6d809cd8c8",
   "metadata": {},
   "source": [
    "### Estimate $\\alpha$ parameters\n",
    "Now, let's use the growth rate data for the tickers in the `basis_tickers::Array{String,1}` array, and data for an unknown (to be specified) ticker to estimate the expansion weights $\\alpha$. Let's start by specifying some ticker that we want to model in the `ticker_to_model::String` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3579930-9cb6-4150-b0bf-f5d3af99a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_model = \"NVDA\"; # you choose a ticker symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dd206-34e4-483f-aab5-4a6a27b9ed03",
   "metadata": {},
   "source": [
    "Next, specify the kernel that we are going to use. In theory, we can choose [any kernel function exported by the `KernelFunctions.jl` package](https://juliagaussianprocesses.github.io/KernelFunctions.jl/stable/). We've picked a few of the standard kernels below; uncomment the one you want to use.\n",
    "In addition to choosing the kernel function, you can also specify a value for the [regularization parameter $\\lambda>0$](https://en.wikipedia.org/wiki/Regularization_(mathematics))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "106e61bc-6783-4f6f-93d6-0b2a96c68a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `SqExponentialKernel` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `SqExponentialKernel` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[15]:2"
     ]
    }
   ],
   "source": [
    "Î» = 0.5; # pick a regularization parameter  (must be > 0)\n",
    "Î³ = 0.5; # pick a scale parameter\n",
    "k = SqExponentialKernel() âˆ˜ ScaleTransform(Î³); # RBF kernel w/a scaling parameter for each basis feature\n",
    "#k = ExponentialKernel();\n",
    "#k = LinearKernel(); # inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f5b4e-bf76-43e9-9270-622fcffdf911",
   "metadata": {},
   "source": [
    "The $\\alpha$ parameters are the expansion weights that appear in the kernel expansion expression: $\\hat{y} = \\sum_{i=1}^{n}\\alpha_{i}\\,k(\\hat{\\mathbf{z}},\\hat{\\mathbf{x}}_{i})$\n",
    "where $\\hat{y}$ denotes the _predicted_ output corresponsing to the (unkown) input feature vector $\\hat{\\mathbf{z}}$. From our theory discussion, we know that the $\\alpha$ coefficients are given by: $\\alpha = \\left(\\mathbf{K}+\\lambda\\,\\mathbf{I}\\right)^{-1}\\mathbf{y}$\n",
    "where the kernel matrix $\\mathbf{K}$ has elements $K_{ij} = k(\\mathbf{v}_{i},\\mathbf{v}_{j})$ computed using the _training data_, the matrix $\\mathbf{I}$ denotes the identity matrix, the vector $\\mathbf{y}$ denotes the _training_ outputs and $\\lambda\\geq{0}$ denotes a regularization parameter. \n",
    "\n",
    "In the code block below, we:\n",
    "* After specifying what dataset we are using (and getting some dimensions), we first estimate the vector of observed outputs $\\mathbf{y}$ for our target firm from _the training data_.\n",
    "* Next, we compute the augmented training data matrix $\\hat{\\mathbf{X}}$ which contains $n^{\\prime}$ _training_ feature vectors. The rows of this matrix hold augmented feature vector instances, where each feature vector holds the observed growth rate for the $b$-firms in the `basis_tickers::Array{Float64,1}` array, and a bias term `1` as the last element. Thus, the augmented training data matrix will be $\\hat{\\mathbf{X}}\\in\\mathbb{R}^{n^{\\prime}\\times{(b+1)}}$\n",
    "* Next, we compute the kernel matrix $\\mathbf{K}\\in\\mathbb{R}^{n^{\\prime}\\times{n}^{\\prime}}$ with elements $K_{ij}=k(\\mathbf{v}_{i},\\mathbf{v}_{j})$, where $\\mathbf{v}_{i}$ and $\\mathbf{v}_{j}$ are $b+1$ dimensional feature vectors. __Hmmm__: I thought $\\mathbf{K} = \\hat{\\mathbf{X}}\\hat{\\mathbf{X}}^{\\top}$. What does it mean when use the kernel functions instead of the data to compute $\\mathbf{K}$?\n",
    "* Finally, we compute the expansion weight vector $\\mathbf{\\alpha}$. This vector will be $\\alpha\\in\\mathbb{R}^{n^{\\prime}}$, where each element is the weight of the similarity between a new feature vector $\\mathbf{z}$, and the training vectors $\\left\\{\\hat{\\mathbf{x}}_{i} \\mid i=1,2,\\dots,n^{\\prime}\\right\\}$ as measured by the kernel function $k(\\hat{\\mathbf{z}},\\hat{\\mathbf{x}}_{i})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b961939-f1f6-4dfb-bb50-865ac9f7145c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[16]:4"
     ]
    }
   ],
   "source": [
    "Î±, XM = let\n",
    "\n",
    "    # setup\n",
    "    D = training; # what data set are we using?\n",
    "    number_of_training_days = size(training,1); # number of days of data in training set\n",
    "    \n",
    "    # 1: compute the training y (output we want model)\n",
    "    j = findfirst(s-> s == ticker_to_model, list_of_all_tickers); # index of the ticker that we want to model\n",
    "    y = D[:,j]; # ticker that we want to model -\n",
    "\n",
    "    # compute the training feature matrix X (instance on rows, training features on cols)\n",
    "    number_of_basis_tickers = length(basis_tickers);\n",
    "    XM = zeros(number_of_training_days, number_of_basis_tickers + 1);\n",
    "    for row âˆˆ 1:number_of_training_days\n",
    "        for col âˆˆ 1:number_of_basis_tickers\n",
    "            j = basis_ticker_lookup[col];\n",
    "            XM[row,col] = D[row,j];\n",
    "        end\n",
    "        XM[row,end] = 1.0; # augmented vector has a 1 at the end\n",
    "    end\n",
    "    \n",
    "    # compute K, etc -\n",
    "    K = zeros(number_of_training_days,number_of_training_days);\n",
    "    for i âˆˆ 1:number_of_training_days\n",
    "        váµ¢ = XM[i,:];\n",
    "        for j âˆˆ 1:number_of_training_days\n",
    "            vâ±¼ = XM[j,:];\n",
    "            K[i,j] = k(váµ¢,vâ±¼);\n",
    "        end\n",
    "    end\n",
    "    IM = Matrix(1.0I,number_of_training_days,number_of_training_days);\n",
    "    \n",
    "    # compute Î± -\n",
    "    Î± = inv(K+Î»*IM)*y;\n",
    "    \n",
    "    # # return -\n",
    "    Î±, XM\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70d50167-1149-4e6e-b381-f077ab62665b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `XM` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `XM` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[17]:1"
     ]
    }
   ],
   "source": [
    "size(XM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b90be-dd3e-4c98-922d-611a7d78e07b",
   "metadata": {},
   "source": [
    "### Predict new output(s) $\\hat{\\mathbf{y}}$\n",
    "Now that we have the weights $\\alpha$, let's compute a prediction of an unknown growth rate trajectory using the `test` data. We'll save the _predicted_ growth rate values in the `yÌ‚::Array{Float64,1}` vector, and the _actual_ growth rate in the `y::Array{Float64,1}` vector. Each element of these vectors corresponds to a trading period (in this case, a day, because we are using daily data).\n",
    "\n",
    "In the code block below, we:\n",
    "* After we specify some dimensions, i.e., the number of days that we will predict, the number of basis tickers, etc we compute the _actual_ growth rate for our choice of the `ticker_to_model::String`. We save this data in the `y::Array{Float64,1}` variable. Note: these observed values are drawn from `test` and are _not_ used in the prediction.\n",
    "* Next, we compute the predicted growth rate values, which we save in the `yÌ‚::Array{Float64,1}` array. For each prediction, we compute the augmented growth rate vector for the basis firms, which we store in the $\\mathbf{z}$-array. Then we compute the weighted sum of the kernel terms $k(\\hat{\\mathbf{z}},\\hat{\\mathbf{x}}_{i})$, which we store in the `tmp::Float64` term. Finally, we store `tmp` in the `yÌ‚::Array{Float64,1}` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d552bbcf-ae56-4f82-aa75-f473a7952fc5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `training` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[18]:5"
     ]
    }
   ],
   "source": [
    "yÌ‚,y = let\n",
    "\n",
    "    # setup -\n",
    "    number_of_prediction_days = 791; # how many days to we want to simulate?\n",
    "    number_of_training_points = size(training,1);\n",
    "    number_of_basis_tickers = length(basis_tickers);\n",
    "    \n",
    "    # compute actual -\n",
    "    j = findfirst(s-> s == ticker_to_model, list_of_all_tickers); # index of ticker\n",
    "    y = zeros(number_of_prediction_days);\n",
    "    for d âˆˆ 1:number_of_prediction_days\n",
    "        y[d] = test[d,j]; # this is what is observed for ticker_to_model\n",
    "    end\n",
    "\n",
    "    # compute predicted return -\n",
    "    yÌ‚ = zeros(number_of_prediction_days);\n",
    "    for d âˆˆ 1:number_of_prediction_days\n",
    "\n",
    "        # generate a *new* feature vector for the basis tickers\n",
    "        z = zeros(number_of_basis_tickers+1);\n",
    "        for ticker âˆˆ 1:number_of_basis_tickers\n",
    "            j = basis_ticker_lookup[ticker];\n",
    "            z[ticker] = test[d,j]; # look up values from test for each basis ticker\n",
    "        end\n",
    "        z[end] = 1.0; # augmented, has a 1 on the end\n",
    "\n",
    "        # compute the prediction yÌ‚ - \n",
    "        tmp = 0.0;\n",
    "        for t âˆˆ 1:number_of_training_points\n",
    "            xâ‚œ = XM[t,:]; # grab the training feature vector\n",
    "            tmp+=Î±[t]*k(z,xâ‚œ); # compare the new basis feature vector to the training vector\n",
    "        end\n",
    "        yÌ‚[d] = tmp;\n",
    "    end\n",
    "    yÌ‚,y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d7143-bf4d-42ac-bd98-7beb879bbf9b",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "`Unhide` the code block below to see how we plotted the observed and the predicted growth rate time series for the `test` dataset for the ticker specified in the `ticker_to_model::String`.\n",
    "* __Summary__: Depending upon the choice of kernel and hyperparameters such as the regularization constant $\\lambda>0$, the predicted time series (red) generally tracks the observed (blue) time-series. However, the individual ticker may exhibit particular dynamics unique to that asset, which may not be reflected in the basis firms. Thus, we _will likely_ miss significant changes that are unique to a particular asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5426f068-934b-4911-8920-4e7cc4f047ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `y` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[19]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "    p = plot(bg=\"gray72\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent); # make an empty plot\n",
    "    plot!(y[1:200], label=\"Actual $(ticker_to_model)\", lw=2, c=:navy)\n",
    "    plot!(yÌ‚[1:200], label=\"Predicted $(ticker_to_model)\", lw=2, c=:red)\n",
    "    \n",
    "    title!(\"Case: Prediction\", fontsize=18)\n",
    "    xlabel!(\"Trading day index (AU)\", fontsize=18)\n",
    "    ylabel!(\"Daily Normalized Growth Rate (AU)\", fontsize=18)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d08277e-720b-4f1c-b7e2-d41437476309",
   "metadata": {},
   "source": [
    "`Unhide` the code block below to see how we plotted the daily growth rate distribution for the `ticker_to_model::String` firm for the `test` data. \n",
    "* __Summary__: These predicted data show the characteristics of a fat-tailed distribution, i.e., skinny in the center wide of the extremes, i.e., they are _not_ normally distributed (dashed-line). However, we capture the distribution with better or worse quality depending upon our choices. One seemingly conserved feature is that the prediction is even more fat-tailed than the observed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd21acd-24cb-42f4-a14b-f5b9bd001fd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `y` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `y` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[20]:3"
     ]
    }
   ],
   "source": [
    "let\n",
    "    p = plot(bg=\"gray72\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent, legend=:topleft); # make an empty plot\n",
    "    density!(y, label=\"Actual $(ticker_to_model)\", lw=2, c=:navy)\n",
    "    density!(yÌ‚, label=\"Predicted $(ticker_to_model)\", lw=2, c=:red)\n",
    "\n",
    "    # build a normal -\n",
    "    d = fit_mle(Normal, y)\n",
    "    plot!(d, ls=:dash, c=:navy, label=\"Normal (obs) $(ticker_to_model)\")\n",
    "\n",
    "    title!(\"Case: Prediction\", fontsize=18)\n",
    "    xlabel!(\"Daily Normalized Growth Rate (AU)\", fontsize=18)\n",
    "    ylabel!(\"Probability density (AU)\", fontsize=18)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ca750-ce68-4432-9021-88e5dff60a01",
   "metadata": {},
   "source": [
    "## Disclaimer and Risks\n",
    "\n",
    "__This content is offered solely for training and informational purposes__. No offer or solicitation to buy or sell securities or derivative products or any investment or trading advice or strategy is made, given, or endorsed by the teaching team.\n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance. Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. Such decisions should be based solely on evaluating your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
